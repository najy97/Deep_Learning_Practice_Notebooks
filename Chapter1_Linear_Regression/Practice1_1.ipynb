{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Practice.1\n",
    "## Forward and Backward Propagation with $y = \\theta x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Dataset Generation\n",
    "NumPy를 이용하여 다음과 같은 x_data, y_data를 만드시오.\n",
    "\n",
    "| Data Index | Study Hour | Math Score |\n",
    "|:-:|:-:|:-:|\n",
    "|0|1|1|\n",
    "|1|2|2|\n",
    "|2|3|3|\n",
    "|3|4|4|\n",
    "|4|5|5|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5] [1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "py_array = [1,2,3,4,5]\n",
    "x_data = np.array(py_array)\n",
    "y_data = np.array(py_array)\n",
    "py_array.append(6)\n",
    "\n",
    "print(x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "    \n",
    "\n",
    "[1 2 3 4 5] [1 2 3 4 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "### Node Implementation\n",
    "다음은 plus node의 예제이다.\n",
    "\n",
    "우리의 model이 $y = \\theta x$였을 때, $(x^{(1)},y^{(1)}) $에 대하여 loss를 구하는 과정은 다음과 같다.\n",
    "\n",
    "Step1: $ z_{1}^{(1)} = \\theta x^{(1)} $\n",
    "\n",
    "Step2: $ z_{2}^{(1)} = y^{(1)} - z_{1}^{(1)} $\n",
    "\n",
    "Step3: $ L^{(1)} = (z_{2}^{(1)})^{2} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 과정을 forward propagation이라하며, 차례대로 곱셈, 뺄셈, 제곱이 이루어진다.\n",
    "\n",
    "그러면 위의 과정 외에 forward-propagation 및 backpropagation을 연습하기 위하여 덧셈, 뺄셈, 곱셈, 제곱에 대한 node를 만들어 각각 forward, backward 기능을 추가해보자. <br><br>\n",
    "\n",
    "forward() method는 input들을 더하여 다음 node로 보내주는 역할을 하며\n",
    "\n",
    "backward() method는 output 쪽에서 propagated된 loss에 대한 gradient를 받아 자신의 gradient를 곱하여 input 쪽으로 넘겨준다. 즉\n",
    "\n",
    "$ z = x + y$에 대하여 $\\frac{\\partial z}{\\partial x} = 1$, $\\frac{\\partial z}{\\partial y} = 1$ 이므로 Chain rule에 의해 Output node 쪽에서 넘어온 dL과 1, 1을 곱해 앞단으로 $1*dL$, $1*dL$을 넘겨주는 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plus node(forward): 7\n",
      "plus node(backward): (5, 5) \n",
      "\n",
      "minus node(forward): -1\n",
      "minus node(backward): (5, -5) \n",
      "\n",
      "multiplication node(forward): 12\n",
      "multiplication node(backward): (20, 15) \n",
      "\n",
      "square node(forward): 9\n",
      "square node(backward): 30 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class plus_node():\n",
    "    def __init__(self):\n",
    "        self.x, self.y, self.z = None, None, None\n",
    "    def forward(self, x, y):\n",
    "        self.x, self.y, self.z = x, y, x + y\n",
    "        return self.z\n",
    "    def backward(self, dL):\n",
    "        return 1*dL, 1*dL\n",
    "class minus_node():\n",
    "    def __init__(self):\n",
    "        self.x, self.y, self.z = None, None, None\n",
    "    def forward(self, x, y):\n",
    "        self.x, self.y, self.z = x, y, x-y\n",
    "        return self.z\n",
    "    def backward(self, dL):\n",
    "        return 1*dL, -1*dL\n",
    "\n",
    "class mul_node():\n",
    "    def __init__(self):\n",
    "        self.x, self.y, self.z = None, None, None\n",
    "    def forward(self, x, y):\n",
    "        self.x, self.y, self.z = x, y, x*y\n",
    "        return self.z\n",
    "    def backward(self, dL):\n",
    "        return self.y*dL, self.x*dL\n",
    "        \n",
    "class square_node():\n",
    "    def __init__(self):\n",
    "        self.x, self.z = None, None\n",
    "    def forward(self, x):\n",
    "        self.x, self.z = x, np.square(x)\n",
    "        return self.z\n",
    "    def backward(self, dL):\n",
    "        return 2*self.x*dL\n",
    "\n",
    "test_plus = plus_node()\n",
    "print(\"plus node(forward):\", test_plus.forward(3, 4))\n",
    "print(\"plus node(backward):\", test_plus.backward(5), '\\n')\n",
    "\n",
    "test_minus = minus_node()\n",
    "print(\"minus node(forward):\", test_minus.forward(3, 4))\n",
    "print(\"minus node(backward):\", test_minus.backward(5), '\\n')\n",
    "\n",
    "test_mul = mul_node()\n",
    "print(\"multiplication node(forward):\", test_mul.forward(3, 4))\n",
    "print(\"multiplication node(backward):\", test_mul.backward(5), '\\n')\n",
    "\n",
    "test_square = square_node()\n",
    "print(\"square node(forward):\", test_square.forward(3))\n",
    "print(\"square node(backward):\", test_square.backward(5), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "plus node(forward): 7 <br>\n",
    "plus node(backward): (5, 5) <br><br>\n",
    "\n",
    "minus node(forward): -1 <br>\n",
    "minus node(backward): (5, -5) <br><br>\n",
    "\n",
    "multiplication node(forward): 12 <br>\n",
    "multiplication node(backward): (20, 15) <br><br> \n",
    "\n",
    "square node(forward): 9 <br>\n",
    "square node(backward): 90 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Forward & Backward Propagation and Parameter Update\n",
    "위의 node들을 이용하여 전체 model을 만들고, forward propagation, backward propagation을 구현하여 $\\theta$를 update하는 코드를 작성하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'theta')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "py_array = [1,2,3,4,5]\n",
    "x_data = np.array(py_array)\n",
    "y_data = np.array(py_array)\n",
    "\n",
    "theta = 0\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "\n",
    "##### Your Code(Model Implmenataion) #####\n",
    "class plus_node():\n",
    "    def __init__(self):\n",
    "        self.x, self.y, self.z = None, None, None\n",
    "    def forward(self, x, y):\n",
    "        self.x, self.y, self.z = x, y, x + y\n",
    "        return self.z\n",
    "    def backward(self, dL):\n",
    "        return 1*dL, 1*dL\n",
    "    \n",
    "class minus_node():\n",
    "    def __init__(self):\n",
    "        self.x, self.y, self.z = None, None, None\n",
    "    def forward(self, x, y):\n",
    "        self.x, self.y, self.z = x, y, x-y\n",
    "        return self.z\n",
    "    def backward(self, dL):\n",
    "        return 1*dL, -1*dL\n",
    "\n",
    "class mul_node():\n",
    "    def __init__(self):\n",
    "        self.x, self.y, self.z = None, None, None\n",
    "    def forward(self, x, y):\n",
    "        self.x, self.y, self.z = x, y, x*y\n",
    "        return self.z\n",
    "    def backward(self, dL):\n",
    "        return self.y*dL, self.x*dL\n",
    "        \n",
    "class square_node():\n",
    "    def __init__(self):\n",
    "        self.x, self.z = None, None\n",
    "    def forward(self, x):\n",
    "        self.x, self.z = x, np.square(x)\n",
    "        return self.z\n",
    "    def backward(self, dL):\n",
    "        return 2*self.x*dL\n",
    "    \n",
    "plus=plus_node()\n",
    "minus = minus_node()\n",
    "mul = mul_node()\n",
    "square = square_node()\n",
    "\n",
    "##### Your Code(Model Implmenataion) #####\n",
    "loss_list = []\n",
    "theta_list = []\n",
    "for i in range(epochs):\n",
    "    z1 = mul.forward(x_data, theta)\n",
    "    z2 = minus.forward(y_data,z1)\n",
    "    loss_list.append(square.forward(z2))\n",
    "    ##### Your Code(Forward Propagation) #####\n",
    "    \n",
    "    ##### Your Code(Backward Propagation) #####\n",
    "    dz2 = square.backward(1)\n",
    "    dy,dz1 = minus.backward(dz2)\n",
    "    dx,dtheta = mul.backward(dz1)\n",
    "    theta = theta - lr*dtheta\n",
    "    theta_list.append(theta)\n",
    "    ##### Your Code(Backward Propagation) #####\n",
    "        \n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax.plot(loss_list)\n",
    "ax.set_title(\"loss\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax.plot(theta_list)\n",
    "ax.set_title(\"theta\")\n",
    "\n",
    "# =============================================================================\n",
    "# Q1) 위의 결과에서 Loss와 theta 모두 부드러운 곡선의 형태가 나타나지 않는 이유를 분석하시오\n",
    "#     Loss의 경우 : (y-theta*x)^2 꼴로 크기가 2차 방정식의 형태로 상승한다.\n",
    "#                   그 다음 epoch에서 x[0]과 비교하므로 loss값이 작은 값 부터 다시 2차 방정식의 형태로 상승한다.\n",
    "#     theta의 경우 : loss값의 기울기에 비례하여 크기가 변하므로 epoch의 시작점 마다 낮은 기울기를 가지는 형태가 된다.\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "<img src=\"./images/1_1_image1.png\" width = 400>\n",
    "\n",
    "<img src=\"./images/1_1_image2.png\" width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1) 위의 결과에서 Loss와 theta 모두 부드러운 곡선의 형태가 나타나지 않는 이유를 분석하시오** <br>\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

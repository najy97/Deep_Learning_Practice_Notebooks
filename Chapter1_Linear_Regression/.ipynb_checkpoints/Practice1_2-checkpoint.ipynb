{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Practice.1-2\n",
    "## Forward and Backward Propagation with $y = \\theta x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 Practice1-2에서는 datapoint마다의 loss로 parameter를 update하지 않고, datapoint 5개의 loss들을 이용하여 cost를 구하고, 이 cost부터 backpropagation으로 parameter를 update하는 과정을 연습한다.\n",
    "\n",
    "먼저 전 Practice의 Dataset Generation 부분과 Node Implmentation 부분을 가져오자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##### Your Code #####\n",
    "x_data = np.\n",
    "y_data = np.\n",
    "##### Your Code #####\n",
    "\n",
    "class plus_node():\n",
    "    def __init__(self):\n",
    "        self.x, self.y, self.z = None, None, None\n",
    "    def forward(self, x, y):\n",
    "        self.x, self.y, self.z = x, y, x + y\n",
    "        return self.z\n",
    "    def backward(self, dL):\n",
    "        return 1*dL, 1*dL\n",
    "\n",
    "class minus_node():\n",
    "    def __init__(self):\n",
    "        self.x, self.y, self.z = None, None, None\n",
    "    def forward(self, x, y):\n",
    "        \n",
    "    def backward(self, dL):\n",
    "        \n",
    "\n",
    "class mul_node():\n",
    "    def __init__(self):\n",
    "        self.x, self.y, self.z = None, None, None\n",
    "    def forward(self, x, y):\n",
    "        \n",
    "    def backward(self, dL):\n",
    "        \n",
    "class square_node():\n",
    "    def __init__(self):\n",
    "        self.x, self.z = None, None\n",
    "    def forward(self, x):\n",
    "        \n",
    "    def backward(self, dL):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 Practice1-1에서 구한 loss들은 다음과 같이 나타낼 수 있다.\n",
    "\n",
    "$L^{(1)}, L^{(2)}, L^{(3)}, L^{(4)}, L^{(5)}$\n",
    "\n",
    "이때 Cost는 다음과 같이 정리된다.\n",
    "\n",
    "$J(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} L^{(i)}$\n",
    "\n",
    "따라서 이번 Practice1-2에서는 $L^{(i)}$들을 구한 뒤, 위의 식을 이용하여 cost를 구하고\n",
    "\n",
    "cost node에서부터 backprogation을 진행한다.\n",
    "\n",
    "먼저 cost를 구하는 node를 만들어보자.\n",
    "\n",
    "이때 Vectorization을 이용하기 위하여 cost_node()는 $L^{(i)}$들을 ndarray로 받아 평균치를 구해 최종 cost를 구할 것이고, backward에서는 datapoint 개수와 일치하는 차원의 ndarray를 return한다.\n",
    "\n",
    "즉, forward()에서는 np.mean()을 이용하고\n",
    "\n",
    "backward()에서는 (N,) ndarray를 만들어 gradient를 return한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cost_node():\n",
    "    def __init__(self):\n",
    "        self.x, self.z = None, None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "    def backward(self):\n",
    "        \n",
    "    \n",
    "c_node = cost_node()\n",
    "test_gradient = np.array([2, 3, 4, 5, 6])\n",
    "print(\"cost node(forward): \", c_node.forward(test_gradient))\n",
    "print(\"cost node(backward): \", c_node.backward())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "cost node(forward):  4.0\n",
    "\n",
    "cost node(backward):  [0.2 0.2 0.2 0.2 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 cost_node를 이용하여 forward propagation, backward propagation을 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 0\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "\n",
    "z1_node = mul_node()\n",
    "z2_node = minus_node()\n",
    "z3_node = square_node()\n",
    "c_node = cost_node()\n",
    "\n",
    "loss_list = []\n",
    "theta_list = []\n",
    "for i in range(epochs):\n",
    "    \n",
    "    gradient_np = np.empty(0)\n",
    "    theta_np = np.empty(0)\n",
    "    \n",
    "    for i in range(len(x_data)): \n",
    "        ##### Your Code(Forward Propagation) #####\n",
    "\n",
    "\n",
    "        ##### Your Code(Forward Propagation) #####\n",
    "        gradient_np = np.append(gradient_np, np.array([z3]))\n",
    "        \n",
    "    cost = c_node.forward(gradient_np)\n",
    "    loss_list.append(cost)\n",
    "    dz = c_node.backward()\n",
    "    \n",
    "    for i in range(len(x_data)):\n",
    "        ##### Your Code(Forward Propagation) #####\n",
    "\n",
    "\n",
    "        ##### Your Code(Forward Propagation) #####\n",
    "        theta_np = np.append(theta_np, np.array([dtheta]))\n",
    "    theta = theta - lr*np.mean(theta_np)\n",
    "    theta_list.append(theta)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax.plot(loss_list)\n",
    "ax.set_title(\"loss\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "ax.plot(theta_list)\n",
    "ax.set_title(r\"$\\theta$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "<img src=\"./images/1_2_image1.png\" width = 400>\n",
    "<img src=\"./images/1_2_image2.png\" width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1)Practice1-1에서부터 곡선으로 바뀐 이유를 분석하시오.**\n",
    "\n",
    "**Q2)LR이 각각 0.001, 0.003, 0.01, 0.03, 0.1, 0.3일때의 결과를 확인하고 **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

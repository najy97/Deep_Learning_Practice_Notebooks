{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Practice.2-1\n",
    "## Binary Classification with Linear Regression\n",
    "\n",
    "이번 practice에서는 기존에 배웠던 linear regression을 이용하여 binary classification을 하는 방법과 이에 대한 단점에 대해 알아본다.\n",
    "\n",
    "먼저 binary classification을 위하여 학생들의 공부시간에 대한 Pass/No Pass dataset을 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass/No Pass 학생을 각각 50명씩 만들도록 세팅\n",
    "n_P, n_NP = 50, 50\n",
    "# 학생들의 최소/최대 공부시간을 hour_m, hour_M으로 만들고 decision boundary를 hour_b로 만듦\n",
    "# 즉, NP학생들은 2~4시간 공부시간을 가지고 P학생들은 4~6시간의 공부시간을 가짐\n",
    "hour_m, hour_b, hour_M = 2, 4, 6\n",
    "\n",
    "##### Your Code(Dataset Generation/Start) #####\n",
    "study_hour_P = np.random.uniform(low = , high = , size = (n_P,))\n",
    "study_hour_NP = np.random.uniform(low = , high = , size = (n_NP,))\n",
    "\n",
    "# P, NP학생들의 y값들은 각각 1, 0으로 만들어줌\n",
    "result_P = np.ones_like()\n",
    "result_NP = np.zeros_like()\n",
    "##### Your Code(Dataset Generation/End) #####\n",
    "\n",
    "print(\"study_hour_P:\", study_hour_P[:5])\n",
    "print(\"study_hour_NP:\", study_hour_NP[:5])\n",
    "print(\"result_P:\", result_P[:5])\n",
    "print(\"result_NP:\", result_NP[:5], '\\n')\n",
    "print(\"study_hour_P.shape:\", study_hour_P.shape)\n",
    "print(\"study_hour_NP.shape:\", study_hour_NP.shape)\n",
    "print(\"result_P.shape:\", result_P.shape)\n",
    "print(\"result_NP.shape:\", result_NP.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:** <br>\n",
    "study_hour_P: [5.14039354 4.87720303 5.97674768 4.20408962 4.41775351] <br>\n",
    "study_hour_NP: [3.09762701 3.43037873 3.20552675 3.08976637 2.8473096 ] <br>\n",
    "result_P: [1. 1. 1. 1. 1.] <br>\n",
    "result_NP: [0. 0. 0. 0. 0.]  <br>\n",
    " <br>\n",
    "study_hour_P.shape: (50,) <br>\n",
    "study_hour_NP.shape: (50,) <br>\n",
    "result_P.shape: (50,) <br>\n",
    "result_NP.shape: (50,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 data들을 이용하여 실제 learning에 사용할 dataset인 x_data, y_data를 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Your Code(Dataset Generation/Start) #####\n",
    "#x_data = np.hstack()\n",
    "#y_data = np.hstack()\n",
    "##### Your Code(Dataset Generation/End) #####\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,5))\n",
    "ax.plot(x_data[:n_P], y_data[:n_P], 'bo')\n",
    "ax.plot(x_data[n_P:], y_data[n_P:], 'ro')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "<img src=\"./images/2_1_image1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Node Implementation\n",
    "\n",
    "Chapter1 Linear Regression에서 사용된 모든 node들을 불러오자. 이번 practice에서는 Chapter1에서 다뤘던 linear model을 binary classification에 그대로 적용할 것이기 때문에 추가적인 node를 만들어줄 필요는 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Your Code(Node Loading/Start) #####\n",
    "# Required Nodes: mul_node, plus_node, minus_node, square_node, cost_node\n",
    "\n",
    "class plus_node():\n",
    "    def __init__(self):\n",
    "        self.x, self.y, self.z = None, None, None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x, self.y, self.z = x, y, x + y\n",
    "        return self.z\n",
    "    \n",
    "    def backward(self, dL):\n",
    "        return dL, dL\n",
    "    \n",
    "class minus_node():\n",
    "    def __init__(self):\n",
    "        self.x, self.y, self.z = None, None, None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x, self.y, self.z = x, y, x - y\n",
    "        return self.z\n",
    "    \n",
    "    def backward(self, dL):\n",
    "        return dL, -1*dL\n",
    "    \n",
    "class mul_node():\n",
    "    def __init__(self):\n",
    "        self.x, self.y, self.z = None, None, None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x, self.y, self.z = x, y, x*y\n",
    "        return self.z\n",
    "    def backward(self, dL):\n",
    "        return self.y*dL, self.x*dL\n",
    "\n",
    "class square_node():\n",
    "    def __init__(self):\n",
    "        self.x, self.z = None, None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x, self.z = x, x*x\n",
    "        return self.z\n",
    "    \n",
    "    def backward(self, dL):\n",
    "        return 2*self.x*dL\n",
    "        \n",
    "\n",
    "class cost_node():\n",
    "    def __init__(self):\n",
    "        self.x, self.z = None, None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.z = np.mean(self.x)\n",
    "        return self.z\n",
    "    def backward(self):\n",
    "        return 1/len(self.x)*np.ones(shape = (self.x.shape))\n",
    "##### Your Code(Node Loading/End) #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Linear Model Implementation\n",
    "\n",
    "위에서 말했듯이 linear regression model을 그대로 사용할 것이기 때문에 다음과 같은 node들을 implementation해야 한다. 그리고 이번 practice부터는 모든 학생들의 결과를 최대한 비슷하게 맞추기 위해 node의 이름과 backpropagation을 저장하는 variable을 다음과 같이 통일하도록 하자.(forward propagation, backpropagation에 대한 variable 이름을 난해하게 적는 학생들이 있기도하고 visualization을 위해서)\n",
    "\n",
    "<img src=\"./images/2_1_image2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Your Code(Model Implementation/Start) #####\n",
    "Z1_node = \n",
    "Z2_node =\n",
    "Z3_node = \n",
    "L_node = \n",
    "J_node = \n",
    "##### Your Code(Model Implementation/Start) #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Learning Preparation\n",
    "\n",
    "learning을 시작하기 앞서 learning에 필요한 hyperparameter를 정의하고 결과를 visualization하기 위한 list들을 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1, theta0 = 0, 0# theta1, theta0 설정\n",
    "lr = 0.001# learning rate 설정\n",
    "epochs = 50000#총 epoch 설정\n",
    "\n",
    "cost_list = []\n",
    "theta1_list, theta0_list = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    ##### Your Code(Learning Process/Start) #####\n",
    "    Z1 = \n",
    "    Z2 = \n",
    "    Z3 = \n",
    "    L = \n",
    "    J = \n",
    "    \n",
    "    dL = \n",
    "    dZ3 = \n",
    "    dY, dZ2 = \n",
    "    dZ1, dTheta0 = \n",
    "    dTheta1, dX = \n",
    "    #### Your Code(Learning Process/End) #####\n",
    "    \n",
    "    theta1 = theta1 - lr*np.sum(dTheta1)\n",
    "    theta0 = theta0 - lr*np.sum(dTheta0)\n",
    "    \n",
    "    cost_list.append(J)\n",
    "    theta1_list.append(theta1)\n",
    "    theta0_list.append(theta0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize = (12, 8))\n",
    "ax[0].set_title(\"Cost\")\n",
    "ax[1].set_title(r'$\\theta_{1} \\quad and \\quad \\theta_{0}$')\n",
    "ax[0].plot(cost_list)\n",
    "ax[1].plot(theta1_list, label = r'$\\theta_{1}$')\n",
    "ax[1].plot(theta0_list, label = r'$\\theta_{0}$')\n",
    "ax[1].legend(loc = 'upper right', fontsize = 20)\n",
    "\n",
    "x_min, x_max = x_data.min(), x_data.max()\n",
    "y_min, y_max = x_min*theta1 + theta0, x_max*theta1 + theta0\n",
    "x_range = np.linspace(x_min, x_max, 1000)\n",
    "y_range = x_range*theta1 + theta0\n",
    "y_d_idx = np.where(np.abs(y_range - 0.5) == np.min(np.abs(y_range - 0.5)))\n",
    "x_d_val = x_range[y_d_idx]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,5))\n",
    "ax.plot(x_data[:n_P], y_data[:n_P], 'bo')\n",
    "ax.plot(x_data[n_P:], y_data[n_P:], 'ro')\n",
    "ax.plot([x_min, x_max], [y_min, y_max], 'r', linewidth = 2)\n",
    "ax.plot([x_range[y_d_idx], x_range[y_d_idx]], [0, y_range[y_d_idx]], 'purple', linewidth = 3)\n",
    "ax.plot(x_range[y_d_idx], y_range[y_d_idx], 'purple', marker = 'o', markersize = 10)\n",
    "ax.text(x_range[y_d_idx]*1.05, y_range[y_d_idx],\n",
    "        s = \"Decision Boundary:\" + str(np.round(x_range[y_d_idx], 2)),\n",
    "       fontdict = {'color':  'purple', 'fontsize': 20})\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "<img src=\"./images/2_1_image3.png\"> <br>\n",
    "<img src=\"./images/2_1_image4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1) dataset generation 부분에서 정한 decision boundary와 learning 후 우리의 classfier의 decision boundary가 일치함을 확인하시오.** <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Adding Outliers\n",
    "\n",
    "위의 결과에서는 linear regression model로 binary classification이 잘 되는 것처럼 보인다.\n",
    "\n",
    "그러면 이 model의 약점을 알아보기 위해 outlier들을 추가해보자.\n",
    "\n",
    "이 outlier들은 공부시간이 월등히 높고, 합격한 data들이 된다.\n",
    "\n",
    "* 참고로 결과가 제대로 나오지 않을 경우 practice 맨 위의 dataset generation 부분을 다시 실행시키고 아래의 cell을 실행시켜줘야 결과가 제대로 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = 20\n",
    "hour_out_m, hour_out_M = 12, 15\n",
    "##### Your Code(Adding Outliers/Start) #####\n",
    "study_hour_outlier = np.random.uniform(low = , high = , size = )\n",
    "result_outlier = np.ones_like()\n",
    "\n",
    "study_hour_P = np.append()\n",
    "result_P = np.append()\n",
    "\n",
    "x_data = np.hstack()\n",
    "y_data = np.hstack()\n",
    "##### Your Code(Adding Outliers/Start) #####\n",
    "\n",
    "print(\"study_hour_P.shape:\",study_hour_P.shape)\n",
    "print(\"result_P.shape:\", result_P.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,5))\n",
    "ax.plot(x_data[:n_P + n_out], y_data[:n_P + n_out], 'bo')\n",
    "ax.plot(x_data[n_P + n_out:], y_data[n_P + n_out:], 'ro')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:** <br>\n",
    "\n",
    "study_hour_P.shape: (70,)<br>\n",
    "result_P.shape: (70,)<br>\n",
    "<img src=\"./images/2_1_image5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1, theta0 = 0, 0 # theta1, theta0 초기화\n",
    "cost_list = []\n",
    "theta1_list, theta0_list = [], []\n",
    "\n",
    "# 위의 Learning Process를 복사하시오\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize = (12, 8))\n",
    "ax[0].plot(cost_list)\n",
    "ax[1].plot(theta1_list, label = r'$\\theta_{1}$')\n",
    "ax[1].plot(theta0_list, label = r'$\\theta_{0}$')\n",
    "ax[1].legend(loc = 'upper right', fontsize = 20)\n",
    "\n",
    "x_min, x_max = x_data.min(), x_data.max()\n",
    "y_min, y_max = x_min*theta1 + theta0, x_max*theta1 + theta0\n",
    "x_range = np.linspace(x_min, x_max, 1000)\n",
    "y_range = x_range*theta1 + theta0\n",
    "y_d_idx = np.where(np.abs(y_range - 0.5) == np.min(np.abs(y_range - 0.5)))\n",
    "x_d_val = x_range[y_d_idx]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,5))\n",
    "ax.plot(x_data[:n_P + n_out], y_data[:n_P + n_out], 'bo')\n",
    "ax.plot(x_data[n_P + n_out:], y_data[n_P + n_out:], 'ro')\n",
    "ax.plot([x_min, x_max], [y_min, y_max], 'r', linewidth = 2)\n",
    "ax.plot([x_range[y_d_idx], x_range[y_d_idx]], [0, y_range[y_d_idx]], 'purple', linewidth = 3)\n",
    "ax.plot(x_range[y_d_idx], y_range[y_d_idx], 'purple', marker = 'o', markersize = 10)\n",
    "ax.text(x_range[y_d_idx]*1.05, y_range[y_d_idx],\n",
    "        s = \"Decision Boundary:\" + str(np.round(x_range[y_d_idx], 2)),\n",
    "       fontdict = {'color':  'purple', 'fontsize': 20})\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "<img src=\"./images/2_1_image6.png\"> <br>\n",
    "<img src=\"./images/2_1_image7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2) linear regression model을 이용하여 binary classification을 진행했을 때 outlier가 learning에 미치는 결과에 대하여 분석하시오**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
